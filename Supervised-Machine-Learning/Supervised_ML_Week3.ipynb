{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "A supervised learning algorithm to predict the class/category for a given input feature from a finite set of possible outputs. This is done using logistic regression.\n",
    "\n",
    "## Logistic regression\n",
    "A supervised learning algorithm which is used for **binary classification** problems. The algorithm outputs whether for a given set of input, does the input belongs to a particular class or not.\n",
    "$$\n",
    "f_{\\vec{w},b}(\\vec{x}) = g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "where, $z = f_{\\vec{w},b}(\\vec{x}) => \\vec{w}\\vec{x} + b$ and $\\frac{1}{1 + e^{-z}}$  is called as **sigmoid/logistic function**<br>\n",
    "\n",
    "### Sigmoid function\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "A mathematical function of which for a given real value $z$, will output a value between $0$ and $1$. When,\n",
    "1.  $z = 0  =>  \\sigma(z) = 0.5$\n",
    "2.  $z$ reaches positive infinity, $\\sigma{z}$ reaches closer to $1$\n",
    "3.  $z$ reaches negative infinity, $\\sigma{z}$ reaches closer to $0$<br>\n",
    "Plotting the output of sigmoid function gis as a $\"S\"$ curve intersecting the y-axis at 0.5.\n",
    "\n",
    "## Decisoin boundary\n",
    "A value$(z)$ where the sigmoid function is zero. \n",
    "$$\\sigma(z = c) = 0$$\n",
    "For other values of $z$ when $\\sigma(z) >= c$, the modal predicts the input belongs to class 1. If $\\sigma(z) < c$, then the modal predicts the input belongs to the class 0.<br> The decisoin boundary doesn't necessarily need to be 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost functoin ($J(f(x), y)$)\n",
    "\n",
    "In the case of cost function for logistic regression, making use of MSE (Mean Squared Error) is a bad choice. As applying MSE for the logistic regression will give zig-zag convex curve with multiple local minima resulting in the modal getting at any of those minima and doesn't converge to get the optimal values for the modal parameters. So logistic regression requires a suitable cost function which will be suitable for its prediction nature (0 or 1) which is as follows:\n",
    "$$\n",
    "J(f_{\\vec{w}, b}(\\vec{x})) = \\frac{1}{m}\\sum_{i=1}^{n}\\frac{1}{2}L(f_{\\vec{w}, b}(\\vec{x}^{i}), y^{i})\n",
    "$$\n",
    "where the loss $L$ is defined as follows,\n",
    "$$\n",
    "L(f_{\\vec{w}, b}(x^{i}), y^{i}) = \\begin{cases} -log(f_{\\vec{w}, b}(\\vec{x}^{i})), for  \\vec{y}^{i} = 1 \\\\ -log(1 - f_{\\vec{w}, b}(\\vec{x}^{i})), for  \\vec{y}^{i} = 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "1.  **Case 1 (y = 1):** Plotting $f(x^{i})$ against J(f(x^{i}), y^{1}) will give an exponential curve intersecting the x axis at 1. Since the modal outputs value only between 0 and 1, consider the portion  of the curve from $x = 0$ to $x = 1$. Given $y = 1$, and the modal predicts 1, ($\\hat{y} = 1$), the cost will be minimal as the point will be closer to 1 in the curve. When the modal predicts value away from 1, the cost increases as the point will move away from the point 1 along the x-axis.\n",
    "2.  **Case 2 (y = 0):** Plotting $f(x^{i})$ against J(f(x^{i}), y^{1}) will give an exponential curve intersecting the x axis at the origin. Since the modal outputs value only between 0 and 1, consider the portion  of the curve from $x = 0$ to $x = 1$. Given $y = 0$, and the modal predicts 0, ($\\hat{y} = 0$), the cost will be minimal as the point will be closer to 0 in the curve. When the modal predicts value away from 0, the cost increases as the point will move away from the point 0 along the x-axis.<br>\n",
    "\n",
    "This loss function will give a curved plot (loss function vs modal parameter) enabling the modal to converge to find the minima.\n",
    "\n",
    "### Simplified cost function\n",
    "\n",
    "For the purpose of implementation of gradient descent, the above given loss $L$ can be rewritten as follows,\n",
    "$$\n",
    "L(f_{\\vec{w}, b}(x^{i}), y^{i}) = -y^{i}log(f_{\\vec{w}, b}(\\vec{x}^{i})) - (1 - y)log(1 - f_{\\vec{w}, b}(\\vec{x}^{i}))\n",
    "$$\n",
    "when,\n",
    "1.  $y = 1$: Substituting $y = 1$ in the above equation gives us, $L(f_{\\vec{w}, b}(x^{i}), y^{i}) = -log(f_{\\vec{w}, b}(\\vec{x}^{i}))$\n",
    "2.  $y = 0$: Substituting $y = 0$ in the above equation gives us, $L(f_{\\vec{w}, b}(x^{i}), y^{i}) = -log(1 - f_{\\vec{w}, b}(\\vec{x}^{i}))$<br>\n",
    "Plugging the above loss function into the cost function, we get:\n",
    "$$\n",
    "J(f_{\\vec{w}, b}(\\vec{x})) = -\\frac{1}{m}\\sum_{i=1}^{n}[y^{i}log(f_{\\vec{w}, b}(\\vec{x}^{i})) + (1 - y^{i})log(1 - f_{\\vec{w}, b}(\\vec{x}^{i}))]\n",
    "$$\n",
    "The above given is the cost function of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "Similar to linear regression, in order to find the optimal values for the modal's parameters $(w, b)$, perform gradient descent on the modal and update the value of parameter such that the cost will be minimal. The algorithm is as follows,\n",
    "1. Choose a random value for the modal's parameters $(w, b)$\n",
    "2. Find the modal's prediction for the input feature\n",
    "3. Compute the gradient and update the parameter values,\n",
    "$$\n",
    "w = w - \\alpha[\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{i}) - y^{i})x_{j}^{i}]\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha[\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{i}) - y^{i})]\n",
    "$$\n",
    "4. Repeat the above step until the cost is close to zero to find the optimal values of the parameters\n",
    "\n",
    "In the gradient descent formula, the definition of $f(x)$ is as follows, $f_{\\vec{w}, b}(\\vec{x}^{i}) = \\frac{1}{1 + e^{-(wx + b)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting, underfitting and generalization\n",
    "\n",
    "Irrespective of the type of algorithm, regression or classification, the ultimate aim of developing and training an ML algorithm is to make the modal predict correctly for unknown input features with atmost accuracy.\n",
    "Unfortunately, when training an algorithm, there can be situations where the modal underit, overfit or in contrast can generalize weill with unknown data.\n",
    "\n",
    "### Generalization\n",
    "Given a learning algorithm, the modal is trained on the training dataset and it performs very well for a given modal parameters. When we test the same modal with data which are not present in the training dataset, the modal shows predicts the output correctly with utmost accuracy. There can be a small number incorrect predictions but on an overall perspective, the modal generalizes very well to the unknown dataset. This is called as **generalization**.\n",
    "\n",
    "### Underitting\n",
    "Given a learning algorithm, the modal doesn't even perform well on the training set is called as **underfitting**. This can also be termed as **high bias**, because the modal has a preconception that the dataset has a particular relationship which is actually in contrast to the actual relationship leading to fit the dataset poorly even during the training.\n",
    "\n",
    "### Overfitting\n",
    "Given a learning algorithm, the modal performs well on the training dataset but predicts very poorly for testing dataset/unknown data which is called as **overitting**. This is because, for a chosen modal parameter value the modal tries hard enough to predict the exact output as the actual ne giving a cost of zero. And so this will lead to finding the best-fit which is kind of zig-zag and so when the modal encounters an unknown data, the prediction will be incorrect with respect to the zig-zag best-fit line. This scenario is also termed as **high variance**.\n",
    "\n",
    "### Addressing overfitting\n",
    "In order to address the issue of overfitting, consider the following options,\n",
    "1.  **Collect more training data** - As number of training examples increases, the chances of verfitting reduces\n",
    "2.  **Feature selection** - Select the features which are relevent enough for the problem\n",
    "3.  **Regularization** - Reduc the values of the modal's parameters which are irrelevent instead of removing those features. This will make the irrelevent features have least impact on the modal's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and regularization\n",
    "\n",
    "As discussed above, egularization involves reducing the impact of input features on the modal's prediction to avoid overfitting. In practice, the training dataset of a learning algorithm will have a large number of features, say a 100, 200 or even more and so to determine which among those features are relevent and which among those are irrelevent to the modal's prediction beforhand is impossible. So by conventionwe penalize or say reduce the impact of all n features on the modal's output in order to avoid overfitting. This penalizing is done in computing the cost function of a learning algorithm. For linear regression, the cost function with regularization will be as follows,,\n",
    "$$\n",
    "J(\\vec{w}, b) = \\frac{1}{2m}\\sum_{i=1}^{n}(f_{\\vec{w}, b}(\\vec{x}) - y)^2 + \\frac{\\lambda}{2m}\\sum_{i=1}^{m}w_{j}^{2}\n",
    "$$\n",
    "where, $\\lambda$ is called as **regularization parameter**<br>\n",
    "In the process of gradient descent, the algorithm not only tries to reduce the cost to fit the dataset but also to find values of $\\vec{w}$ closer to zero to avoid overfitting.\n",
    "Chooosing the value of $\\lambda$ is similar to finding the optimal value for learning rate($\\alpha$). When regularization parameter is,\n",
    "1.  **$\\lambda$ = 0/too small**: The second term in the cost function gets eliminated and so the modal overfits\n",
    "2.  **$\\lambda$ = too large**: The values of the modal parameter will become almost zero and so the modal underfits. When the values of $\\vec{w}$ becomes zero, the only term left is $b$ leading to poor fit of the dataset\n",
    "3.  **$\\lambda$ = just right**: The model fits the dataset with atmost accuracy and the chances of overfitting reduces\n",
    "\n",
    "### Regularization in linear regression\n",
    "The formulae for simultaneous updates of the parameters of linear regression with the regularization term will be as follow,<br>\n",
    "$$\n",
    "\\begin{cases} w_{j} = w_{j} - \\alpha[\\frac{1}{m}\\sum_{i=1}^{n}(f_{\\vec{w},b}(x^{i}) - y^{i})x^{i}] + \\frac{\\lambda}{m}w_{j} \\\\ \n",
    "b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{n}(f_{\\vec{w},b}(x^{i}) - y^{i})\n",
    "\\end{cases}\n",
    "$$\n",
    "Since regularizing the parameter $b$ doesn't have a significant impact, it's updation formula doesn't change like the other parameters.<br>\n",
    "\n",
    "## Regularization in logistic regression\n",
    "Similar to linear regression, the formulae for simultaneous update of modal parameters are as follows,\n",
    "$$\n",
    "\\begin{cases} w_{j} = w_{j} - \\alpha[\\frac{1}{m}\\sum_{i=1}^{n}(f_{\\vec{w},b}(x^{i}) - y^{i})x^{i}] + \\frac{\\lambda}{m}w_{j} \\\\ \n",
    "b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{n}(f_{\\vec{w},b}(x^{i}) - y^{i})\n",
    "\\end{cases}\n",
    "$$\n",
    "The only differences is that the definitiong of the modal ($f(x)$) changes as the output of the logistic regression is between 0 and 1 coming from a sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

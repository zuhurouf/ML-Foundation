{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "A supervised learning algorithm to predict the class/category for a given input feature from a finite set of possible outputs. This is done using logistic regression.\n",
    "\n",
    "## Logistic regression\n",
    "A supervised learning algorithm which is used for **binary classification** problems. The algorithm outputs whether for a given set of input, does the input belongs to a particular class or not.\n",
    "$$\n",
    "f_{\\vec{w},b}(\\vec{x}) = g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "where, $z = f_{\\vec{w},b}(\\vec{x}) => \\vec{w}\\vec{x} + b$ and $\\frac{1}{1 + e^{-z}}$  is called as **sigmoid/logistic function**<br>\n",
    "\n",
    "### Sigmoid function\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "A mathematical function of which for a given real value $z$, will output a value between $0$ and $1$. When,\n",
    "1.  $z = 0  =>  \\sigma(z) = 0.5$\n",
    "2.  $z$ reaches positive infinity, $\\sigma{z}$ reaches closer to $1$\n",
    "3.  $z$ reaches negative infinity, $\\sigma{z}$ reaches closer to $0$<br>\n",
    "Plotting the output of sigmoid function gis as a $\"S\"$ curve intersecting the y-axis at 0.5.\n",
    "\n",
    "## Decisoin boundary\n",
    "A value$(z)$ where the sigmoid function is zero. \n",
    "$$\\sigma(z = c) = 0$$\n",
    "For other values of $z$ when $\\sigma(z) >= c$, the modal predicts the input belongs to class 1. If $\\sigma(z) < c$, then the modal predicts the input belongs to the class 0.<br> The decisoin boundary doesn't necessarily need to be 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost functoin ($J(f(x), y)$)\n",
    "\n",
    "In the case of cost function for logistic regression, making use of MSE (Mean Squared Error) is a bad choice. As applying MSE for the logistic regression will give zig-zag convex curve with multiple local minima resulting in the modal getting at any of those minima and doesn't converge to get the optimal values for the modal parameters. So logistic regression requires a suitable cost function which will be suitable for its prediction nature (0 or 1) which is as follows:\n",
    "$$\n",
    "J(f_{\\vec{w}, b}(\\vec{x})) = \\frac{1}{m}\\sum_{i=1}^{n}\\frac{1}{2}L(f_{\\vec{w}, b}(\\vec{x}^{i}), y^{i})\n",
    "$$\n",
    "where the loss $L$ is defined as follows,\n",
    "$$\n",
    "L(f_{\\vec{w}, b}(x^{i}), y^{i}) = \\begin{cases} -log(f_{\\vec{w}, b}(\\vec{x}^{i})), for  \\vec{y}^{i} = 1 \\\\ -log(1 - f_{\\vec{w}, b}(\\vec{x}^{i})), for  \\vec{y}^{i} = 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "1.  **Case 1 (y = 1):** Plotting $f(x^{i})$ against J(f(x^{i}), y^{1}) will give an exponential curve intersecting the x axis at 1. Since the modal outputs value only between 0 and 1, consider the portion  of the curve from $x = 0$ to $x = 1$. Given $y = 1$, and the modal predicts 1, ($\\hat{y} = 1$), the cost will be minimal as the point will be closer to 1 in the curve. When the modal predicts value away from 1, the cost increases as the point will move away from the point 1 along the x-axis.\n",
    "2.  **Case 2 (y = 0):** Plotting $f(x^{i})$ against J(f(x^{i}), y^{1}) will give an exponential curve intersecting the x axis at the origin. Since the modal outputs value only between 0 and 1, consider the portion  of the curve from $x = 0$ to $x = 1$. Given $y = 0$, and the modal predicts 0, ($\\hat{y} = 0$), the cost will be minimal as the point will be closer to 0 in the curve. When the modal predicts value away from 0, the cost increases as the point will move away from the point 0 along the x-axis.<br>\n",
    "\n",
    "This loss function will give a curved plot (loss function vs modal parameter) enabling the modal to converge to find the minima.\n",
    "\n",
    "### Simplified cost function\n",
    "\n",
    "For the purpose of implementation of gradient descent, the above given loss $L$ can be rewritten as follows,\n",
    "$$\n",
    "L(f_{\\vec{w}, b}(x^{i}), y^{i}) = -y^{i}log(f_{\\vec{w}, b}(\\vec{x}^{i})) - (1 - y)log(1 - f_{\\vec{w}, b}(\\vec{x}^{i}))\n",
    "$$\n",
    "when,\n",
    "1.  $y = 1$: Substituting $y = 1$ in the above equation gives us, $L(f_{\\vec{w}, b}(x^{i}), y^{i}) = -log(f_{\\vec{w}, b}(\\vec{x}^{i}))$\n",
    "2.  $y = 0$: Substituting $y = 0$ in the above equation gives us, $L(f_{\\vec{w}, b}(x^{i}), y^{i}) = -log(1 - f_{\\vec{w}, b}(\\vec{x}^{i}))$<br>\n",
    "Plugging the above loss function into the cost function, we get:\n",
    "$$\n",
    "J(f_{\\vec{w}, b}(\\vec{x})) = -\\frac{1}{m}\\sum_{i=1}^{n}[y^{i}log(f_{\\vec{w}, b}(\\vec{x}^{i})) + (1 - y^{i})log(1 - f_{\\vec{w}, b}(\\vec{x}^{i}))]\n",
    "$$\n",
    "The above given is the cost function of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
